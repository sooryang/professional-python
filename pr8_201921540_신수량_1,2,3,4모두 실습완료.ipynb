{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8주차 1,2,3,4 모두 다 했습니다. 신수량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 문서(word tokenize된 결과)에 대해 feature set을 dictionary 형태로 구성해서 반환\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words]) #있는 단어들에 대해 True로 표시, 없는 단어는 표시 안 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': True, 'quick': True, 'brown': True, 'fox': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(['the', 'quick', 'brown', 'fox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review count: 2000\n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "['neg', 'pos']\n",
      "\"neg\" reviews: 1000\n",
      "\"pos\" reviews: 1000\n",
      "id: neg/cv000_29416.txt\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5966332cdf6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#첫번째 문서의 내용을 500자까지만 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#첫번째 문서를 word tokenize한 결과 중 앞 열 단어\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m             \u001b[1;31m# Check if it's in the cache.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mslice_bounds\u001b[1;34m(sequence, slice_obj, allow_step)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstop\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[1;31m# Use iterate_from to extract it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index out of range'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[0;32m    308\u001b[0m                 \u001b[1;34m'block reader %s() should return list or tuple.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m_read_sent_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 [\n\u001b[0;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 ]\n\u001b[0;32m    145\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1023\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1024\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1015\u001b[1;33m         \u001b[0mresource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1016\u001b[0m         \u001b[1;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m         \u001b[1;31m# the object by modifying our own __dict__ and __class__ to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
    "print(movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "print(movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
    "print('\"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
    "print('\"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
    "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
    "print('id:', fileid)\n",
    "print(movie_reviews.raw(fileid)[:500]) #첫번째 문서의 내용을 500자까지만 출력\n",
    "print(movie_reviews.sents(fileid)[:2]) #첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\n",
    "print(movie_reviews.words(fileid)[:10]) #첫번째 문서를 word tokenize한 결과 중 앞 열 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['neg', 'pos'])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#feature 추출을 위한 함수로, 앞서 정의한 bag_of_words()를 사용\n",
    "def label_feats_from_corpus(corp, feature_detector=bag_of_words): \n",
    "    label_feats = collections.defaultdict(list) # container 초기화\n",
    "    for label in corp.categories(): #''neg', 'pos' 각 label에 대해\n",
    "        for fileid in corp.fileids(categories=[label]): #각 label에 해당하는 문서들에 대해\n",
    "            feats = feature_detector(corp.words(fileids=[fileid])) #주어진 문서를 bag_of_words feature로 변환\n",
    "            label_feats[label].append(feats) #container에 feature 추가\n",
    "    return label_feats\n",
    "\n",
    "lfeats = label_feats_from_corpus(movie_reviews)\n",
    "print(lfeats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words model을 이용한 feature 추출 첫번째 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lfeats['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot, :, two, teen, couples, go, to, a, church, party, ,, drink, and, then, drive, ., they, get, into, an, accident, one, of, the, guys, dies, but, his, girlfriend, continues, see, him, in, her, life, has, nightmares, what, \\', s, deal, ?, watch, movie, \", sorta, find, out, critique, mind, -, fuck, for, generation, that, touches, on, very, cool, idea, presents, it, bad, package, which, is, makes, this, review, even, harder, write, since, i, generally, applaud, films, attempt, break, mold, mess, with, your, head, such, (, lost, highway, &, memento, ), there, are, good, ways, making, all, types, these, folks, just, didn, t, snag, correctly, seem, have, taken, pretty, neat, concept, executed, terribly, so, problems, well, its, main, problem, simply, too, jumbled, starts, off, normal, downshifts, fantasy, world, you, as, audience, member, no, going, dreams, characters, coming, back, from, dead, others, who, look, like, strange, apparitions, disappearances, looooot, chase, scenes, tons, weird, things, happen, most, not, explained, now, personally, don, trying, unravel, film, every, when, does, give, me, same, clue, over, again, kind, fed, up, after, while, biggest, obviously, got, big, secret, hide, seems, want, completely, until, final, five, minutes, do, make, entertaining, thrilling, or, engaging, meantime, really, sad, part, arrow, both, dig, flicks, we, actually, figured, by, half, way, point, strangeness, did, start, little, bit, sense, still, more, guess, bottom, line, movies, should, always, sure, before, given, password, enter, understanding, mean, showing, melissa, sagemiller, running, away, visions, about, 20, throughout, plain, lazy, !, okay, people, chasing, know, need, how, giving, us, different, offering, further, insight, down, apparently, studio, took, director, chopped, themselves, shows, might, ve, been, decent, here, somewhere, suits, decided, turning, music, video, edge, would, actors, although, wes, bentley, seemed, be, playing, exact, character, he, american, beauty, only, new, neighborhood, my, kudos, holds, own, entire, feeling, unraveling, overall, doesn, stick, because, entertain, confusing, rarely, excites, feels, redundant, runtime, despite, ending, explanation, craziness, came, oh, horror, slasher, flick, packaged, someone, assuming, genre, hot, kids, also, wrapped, production, years, ago, sitting, shelves, ever, whatever, skip, where, joblo, nightmare, elm, street, 3, 7, /, 10, blair, witch, 2, crow, 9, salvation, 4, stir, echoes, 8'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(lfeats['neg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plot': True,\n",
       " ':': True,\n",
       " 'two': True,\n",
       " 'teen': True,\n",
       " 'couples': True,\n",
       " 'go': True,\n",
       " 'to': True,\n",
       " 'a': True,\n",
       " 'church': True,\n",
       " 'party': True,\n",
       " ',': True,\n",
       " 'drink': True,\n",
       " 'and': True,\n",
       " 'then': True,\n",
       " 'drive': True,\n",
       " '.': True,\n",
       " 'they': True,\n",
       " 'get': True,\n",
       " 'into': True,\n",
       " 'an': True,\n",
       " 'accident': True,\n",
       " 'one': True,\n",
       " 'of': True,\n",
       " 'the': True,\n",
       " 'guys': True,\n",
       " 'dies': True,\n",
       " 'but': True,\n",
       " 'his': True,\n",
       " 'girlfriend': True,\n",
       " 'continues': True,\n",
       " 'see': True,\n",
       " 'him': True,\n",
       " 'in': True,\n",
       " 'her': True,\n",
       " 'life': True,\n",
       " 'has': True,\n",
       " 'nightmares': True,\n",
       " 'what': True,\n",
       " \"'\": True,\n",
       " 's': True,\n",
       " 'deal': True,\n",
       " '?': True,\n",
       " 'watch': True,\n",
       " 'movie': True,\n",
       " '\"': True,\n",
       " 'sorta': True,\n",
       " 'find': True,\n",
       " 'out': True,\n",
       " 'critique': True,\n",
       " 'mind': True,\n",
       " '-': True,\n",
       " 'fuck': True,\n",
       " 'for': True,\n",
       " 'generation': True,\n",
       " 'that': True,\n",
       " 'touches': True,\n",
       " 'on': True,\n",
       " 'very': True,\n",
       " 'cool': True,\n",
       " 'idea': True,\n",
       " 'presents': True,\n",
       " 'it': True,\n",
       " 'bad': True,\n",
       " 'package': True,\n",
       " 'which': True,\n",
       " 'is': True,\n",
       " 'makes': True,\n",
       " 'this': True,\n",
       " 'review': True,\n",
       " 'even': True,\n",
       " 'harder': True,\n",
       " 'write': True,\n",
       " 'since': True,\n",
       " 'i': True,\n",
       " 'generally': True,\n",
       " 'applaud': True,\n",
       " 'films': True,\n",
       " 'attempt': True,\n",
       " 'break': True,\n",
       " 'mold': True,\n",
       " 'mess': True,\n",
       " 'with': True,\n",
       " 'your': True,\n",
       " 'head': True,\n",
       " 'such': True,\n",
       " '(': True,\n",
       " 'lost': True,\n",
       " 'highway': True,\n",
       " '&': True,\n",
       " 'memento': True,\n",
       " ')': True,\n",
       " 'there': True,\n",
       " 'are': True,\n",
       " 'good': True,\n",
       " 'ways': True,\n",
       " 'making': True,\n",
       " 'all': True,\n",
       " 'types': True,\n",
       " 'these': True,\n",
       " 'folks': True,\n",
       " 'just': True,\n",
       " 'didn': True,\n",
       " 't': True,\n",
       " 'snag': True,\n",
       " 'correctly': True,\n",
       " 'seem': True,\n",
       " 'have': True,\n",
       " 'taken': True,\n",
       " 'pretty': True,\n",
       " 'neat': True,\n",
       " 'concept': True,\n",
       " 'executed': True,\n",
       " 'terribly': True,\n",
       " 'so': True,\n",
       " 'problems': True,\n",
       " 'well': True,\n",
       " 'its': True,\n",
       " 'main': True,\n",
       " 'problem': True,\n",
       " 'simply': True,\n",
       " 'too': True,\n",
       " 'jumbled': True,\n",
       " 'starts': True,\n",
       " 'off': True,\n",
       " 'normal': True,\n",
       " 'downshifts': True,\n",
       " 'fantasy': True,\n",
       " 'world': True,\n",
       " 'you': True,\n",
       " 'as': True,\n",
       " 'audience': True,\n",
       " 'member': True,\n",
       " 'no': True,\n",
       " 'going': True,\n",
       " 'dreams': True,\n",
       " 'characters': True,\n",
       " 'coming': True,\n",
       " 'back': True,\n",
       " 'from': True,\n",
       " 'dead': True,\n",
       " 'others': True,\n",
       " 'who': True,\n",
       " 'look': True,\n",
       " 'like': True,\n",
       " 'strange': True,\n",
       " 'apparitions': True,\n",
       " 'disappearances': True,\n",
       " 'looooot': True,\n",
       " 'chase': True,\n",
       " 'scenes': True,\n",
       " 'tons': True,\n",
       " 'weird': True,\n",
       " 'things': True,\n",
       " 'happen': True,\n",
       " 'most': True,\n",
       " 'not': True,\n",
       " 'explained': True,\n",
       " 'now': True,\n",
       " 'personally': True,\n",
       " 'don': True,\n",
       " 'trying': True,\n",
       " 'unravel': True,\n",
       " 'film': True,\n",
       " 'every': True,\n",
       " 'when': True,\n",
       " 'does': True,\n",
       " 'give': True,\n",
       " 'me': True,\n",
       " 'same': True,\n",
       " 'clue': True,\n",
       " 'over': True,\n",
       " 'again': True,\n",
       " 'kind': True,\n",
       " 'fed': True,\n",
       " 'up': True,\n",
       " 'after': True,\n",
       " 'while': True,\n",
       " 'biggest': True,\n",
       " 'obviously': True,\n",
       " 'got': True,\n",
       " 'big': True,\n",
       " 'secret': True,\n",
       " 'hide': True,\n",
       " 'seems': True,\n",
       " 'want': True,\n",
       " 'completely': True,\n",
       " 'until': True,\n",
       " 'final': True,\n",
       " 'five': True,\n",
       " 'minutes': True,\n",
       " 'do': True,\n",
       " 'make': True,\n",
       " 'entertaining': True,\n",
       " 'thrilling': True,\n",
       " 'or': True,\n",
       " 'engaging': True,\n",
       " 'meantime': True,\n",
       " 'really': True,\n",
       " 'sad': True,\n",
       " 'part': True,\n",
       " 'arrow': True,\n",
       " 'both': True,\n",
       " 'dig': True,\n",
       " 'flicks': True,\n",
       " 'we': True,\n",
       " 'actually': True,\n",
       " 'figured': True,\n",
       " 'by': True,\n",
       " 'half': True,\n",
       " 'way': True,\n",
       " 'point': True,\n",
       " 'strangeness': True,\n",
       " 'did': True,\n",
       " 'start': True,\n",
       " 'little': True,\n",
       " 'bit': True,\n",
       " 'sense': True,\n",
       " 'still': True,\n",
       " 'more': True,\n",
       " 'guess': True,\n",
       " 'bottom': True,\n",
       " 'line': True,\n",
       " 'movies': True,\n",
       " 'should': True,\n",
       " 'always': True,\n",
       " 'sure': True,\n",
       " 'before': True,\n",
       " 'given': True,\n",
       " 'password': True,\n",
       " 'enter': True,\n",
       " 'understanding': True,\n",
       " 'mean': True,\n",
       " 'showing': True,\n",
       " 'melissa': True,\n",
       " 'sagemiller': True,\n",
       " 'running': True,\n",
       " 'away': True,\n",
       " 'visions': True,\n",
       " 'about': True,\n",
       " '20': True,\n",
       " 'throughout': True,\n",
       " 'plain': True,\n",
       " 'lazy': True,\n",
       " '!': True,\n",
       " 'okay': True,\n",
       " 'people': True,\n",
       " 'chasing': True,\n",
       " 'know': True,\n",
       " 'need': True,\n",
       " 'how': True,\n",
       " 'giving': True,\n",
       " 'us': True,\n",
       " 'different': True,\n",
       " 'offering': True,\n",
       " 'further': True,\n",
       " 'insight': True,\n",
       " 'down': True,\n",
       " 'apparently': True,\n",
       " 'studio': True,\n",
       " 'took': True,\n",
       " 'director': True,\n",
       " 'chopped': True,\n",
       " 'themselves': True,\n",
       " 'shows': True,\n",
       " 'might': True,\n",
       " 've': True,\n",
       " 'been': True,\n",
       " 'decent': True,\n",
       " 'here': True,\n",
       " 'somewhere': True,\n",
       " 'suits': True,\n",
       " 'decided': True,\n",
       " 'turning': True,\n",
       " 'music': True,\n",
       " 'video': True,\n",
       " 'edge': True,\n",
       " 'would': True,\n",
       " 'actors': True,\n",
       " 'although': True,\n",
       " 'wes': True,\n",
       " 'bentley': True,\n",
       " 'seemed': True,\n",
       " 'be': True,\n",
       " 'playing': True,\n",
       " 'exact': True,\n",
       " 'character': True,\n",
       " 'he': True,\n",
       " 'american': True,\n",
       " 'beauty': True,\n",
       " 'only': True,\n",
       " 'new': True,\n",
       " 'neighborhood': True,\n",
       " 'my': True,\n",
       " 'kudos': True,\n",
       " 'holds': True,\n",
       " 'own': True,\n",
       " 'entire': True,\n",
       " 'feeling': True,\n",
       " 'unraveling': True,\n",
       " 'overall': True,\n",
       " 'doesn': True,\n",
       " 'stick': True,\n",
       " 'because': True,\n",
       " 'entertain': True,\n",
       " 'confusing': True,\n",
       " 'rarely': True,\n",
       " 'excites': True,\n",
       " 'feels': True,\n",
       " 'redundant': True,\n",
       " 'runtime': True,\n",
       " 'despite': True,\n",
       " 'ending': True,\n",
       " 'explanation': True,\n",
       " 'craziness': True,\n",
       " 'came': True,\n",
       " 'oh': True,\n",
       " 'horror': True,\n",
       " 'slasher': True,\n",
       " 'flick': True,\n",
       " 'packaged': True,\n",
       " 'someone': True,\n",
       " 'assuming': True,\n",
       " 'genre': True,\n",
       " 'hot': True,\n",
       " 'kids': True,\n",
       " 'also': True,\n",
       " 'wrapped': True,\n",
       " 'production': True,\n",
       " 'years': True,\n",
       " 'ago': True,\n",
       " 'sitting': True,\n",
       " 'shelves': True,\n",
       " 'ever': True,\n",
       " 'whatever': True,\n",
       " 'skip': True,\n",
       " 'where': True,\n",
       " 'joblo': True,\n",
       " 'nightmare': True,\n",
       " 'elm': True,\n",
       " 'street': True,\n",
       " '3': True,\n",
       " '7': True,\n",
       " '/': True,\n",
       " '10': True,\n",
       " 'blair': True,\n",
       " 'witch': True,\n",
       " '2': True,\n",
       " 'crow': True,\n",
       " '9': True,\n",
       " 'salvation': True,\n",
       " '4': True,\n",
       " 'stir': True,\n",
       " 'echoes': True,\n",
       " '8': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfeats['neg'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words model을 이용 feature 추출 두번째 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "# category 별로 fileid를 추출하고, 해당 fileid에 대해 문서의 word tokenize된 결과를 가져와서 \n",
    "# documents 집합을 구성\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "#nltk FreqDist 함수를 이용하여 단어별로 빈도 수를 계산\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot 1513\n",
      ": 3042\n",
      "two 1911\n",
      "teen 151\n",
      "couples 27\n",
      "go 1113\n",
      "to 31937\n",
      "a 38106\n",
      "church 69\n",
      "party 183\n"
     ]
    }
   ],
   "source": [
    "for word in list(all_words)[:10]:\n",
    "    print(word, all_words[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 77717\n",
      "the 76529\n",
      ". 65876\n",
      "a 38106\n",
      "and 35576\n",
      "of 34123\n",
      "to 31937\n",
      "' 30585\n",
      "is 25195\n",
      "in 21822\n",
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in']\n"
     ]
    }
   ],
   "source": [
    "sorted_features = sorted(all_words, key=all_words.get, reverse=True)\n",
    "for word in sorted_features[:10]:\n",
    "    print(word, all_words[word])\n",
    "print(sorted_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', '?', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', ':', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', ';', 'first', '--', 'after', 'see', '!', 'way', 'because', 'make', 'life']\n"
     ]
    }
   ],
   "source": [
    "word_features = sorted_features[:2000] #빈도가 높은 상위 2000개의 단어만 추출하여 features를 구성\n",
    "\n",
    "print(word_features[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 document를 feature로 변환하는 함수, word_features를 사용\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words) #2000개의 단어에 대해 True, False로 입력\n",
    "    return features\n",
    "#위에서 만든 documents 집합에 대해 feature set을 생성\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "pos\n",
      "{',': True, 'the': True, '.': True, 'a': True, 'and': True, 'of': True, 'to': True, \"'\": True, 'is': True, 'in': True, 's': True, '\"': False, 'it': True, 'that': True, '-': True, ')': False, '(': False, 'as': True, 'with': True, 'for': True, 'his': True, 'this': True, 'film': True, 'i': False, 'he': True, 'but': True, 'on': True, 'are': True, 't': True, 'by': True, 'be': True, 'one': True, 'movie': True, 'an': True, 'who': True, 'not': False, 'you': True, 'from': True, 'at': True, 'was': True, 'have': True, 'they': True, 'has': True, 'her': True, 'all': True, '?': True, 'there': True, 'like': False, 'so': True, 'out': True, 'about': False, 'up': True, 'more': False, 'what': True, 'when': True, 'which': True, 'or': False, 'she': True, 'their': True, ':': False, 'some': False, 'just': False, 'can': False, 'if': False, 'we': True, 'him': True, 'into': True, 'even': True, 'only': True, 'than': False, 'no': True, 'good': True, 'time': False, 'most': True, 'its': True, 'will': True, 'story': True, 'would': False, 'been': True, 'much': False, 'character': True, 'also': False, 'get': True, 'other': False, 'do': True, 'two': True, 'well': True, 'them': False, 'very': True, 'characters': True, ';': True, 'first': False, '--': False, 'after': False, 'see': False, '!': False, 'way': False, 'because': True, 'make': True, 'life': False, 'off': False, 'too': False, 'any': True, 'does': True, 'really': False, 'had': True, 'while': False, 'films': False, 'how': False, 'plot': True, 'little': False, 'where': True, 'people': False, 'over': True, 'could': True, 'then': True, 'me': False, 'scene': False, 'man': True, 'bad': False, 'my': False, 'never': True, 'being': False, 'best': False, 'these': True, 'don': True, 'new': False, 'doesn': True, 'scenes': False, 'many': True, 'director': False, 'such': True, 'know': True, 'were': False, 'movies': False, 'through': True, 'here': True, 'action': False, 'great': False, 're': False, 'another': False, 'love': False, 'go': False, 'made': False, 'us': False, 'big': False, 'end': True, 'something': False, 'back': False, '*': False, 'still': True, 'world': False, 'seems': False, 'work': False, 'those': False, 'makes': False, 'now': False, 'before': True, 'however': True, 'between': True, 'few': False, '/': False, 'down': False, 'every': False, 'though': False, 'better': True, 'real': False, 'audience': False, 'enough': False, 'seen': False, 'take': True, 'around': False, 'both': False, 'going': False, 'year': False, 'performance': True, 'why': True, 'should': True, 'role': True, 'isn': False, 'same': True, 'old': False, 'gets': True, 'your': False, 'may': True, 'things': True, 'think': False, 'years': True, 'last': False, 'comedy': False, 'funny': False, 'actually': False, 've': False, 'long': False, 'look': False, 'almost': False, 'own': True, 'thing': False, 'fact': False, 'nothing': False, 'say': False, 'right': False, 'john': False, 'although': False, 'played': False, 'find': False, 'script': False, 'come': False, 'ever': True, 'cast': False, 'since': False, 'did': True, 'star': False, 'plays': True, 'young': False, 'show': False, 'comes': True, 'm': False, 'part': False, 'original': False, 'actors': False, 'screen': False, 'without': True, 'again': False, 'acting': False, 'three': False, 'day': False, 'each': False, 'point': False, 'lot': False, 'least': False, 'takes': True, 'guy': False, 'quite': False, 'himself': False, 'away': False, 'during': True, 'family': False, 'effects': False, 'course': False, 'goes': False, 'minutes': False, 'interesting': False, 'might': False, 'far': False, 'high': False, 'rather': False, 'once': False, 'must': False, 'anything': False, 'place': False, 'set': False, 'yet': True, 'watch': False, 'd': False, 'making': False, 'our': False, 'wife': True, 'hard': False, 'always': False, 'fun': False, 'didn': True, 'll': False, 'seem': False, 'special': False, 'bit': False, 'times': True, 'trying': False, 'hollywood': False, 'instead': False, 'give': False, 'want': False, 'picture': False, 'kind': False, 'american': False, 'job': False, 'sense': True, 'woman': False, 'home': False, 'having': False, 'series': False, 'actor': False, 'probably': True, 'help': False, 'half': False, 'along': True, 'men': False, 'everything': False, 'pretty': False, 'becomes': False, 'sure': False, 'black': False, 'together': True, 'dialogue': False, 'money': False, 'become': False, 'gives': False, 'given': False, 'looking': False, 'whole': False, 'watching': False, 'father': False, '`': False, 'feel': False, 'everyone': False, 'music': False, 'wants': True, 'sex': True, 'less': False, 'done': False, 'horror': False, 'got': False, 'death': True, 'perhaps': False, 'city': False, 'next': False, 'especially': False, 'play': False, 'girl': False, 'mind': True, '10': False, 'moments': False, 'looks': False, 'completely': False, '2': False, 'reason': False, 'mother': False, 'whose': True, 'line': False, 'night': False, 'human': False, 'until': False, 'rest': False, 'performances': True, 'different': True, 'evil': True, 'small': False, 'james': False, 'simply': False, 'couple': False, 'put': False, 'let': False, 'anyone': False, 'ending': True, 'case': True, 'several': False, 'dead': False, 'michael': False, 'left': False, 'thought': False, 'school': False, 'shows': False, 'humor': False, 'true': False, 'lost': False, 'written': False, 'itself': False, 'friend': False, 'entire': False, 'getting': False, 'town': False, 'turns': False, 'soon': False, 'someone': False, 'second': False, 'main': True, 'stars': False, 'found': True, 'use': True, 'problem': False, 'friends': False, 'tv': False, 'top': False, 'name': False, 'begins': False, 'called': False, 'based': True, 'comic': False, 'david': False, 'head': False, 'else': False, 'idea': False, 'either': False, 'wrong': True, 'unfortunately': False, 'later': False, 'final': False, 'hand': False, 'alien': False, 'house': False, 'group': True, 'full': False, 'used': True, 'tries': False, 'often': True, 'against': False, 'war': False, 'sequence': False, 'keep': False, 'turn': False, 'playing': False, 'boy': False, 'behind': False, 'named': False, 'certainly': False, 'live': False, 'believe': False, 'under': False, 'works': True, 'relationship': False, 'face': False, 'hour': False, 'run': False, 'style': False, 'said': False, 'despite': False, 'person': False, 'finally': False, 'shot': False, 'book': False, 'doing': False, 'tell': False, 'maybe': False, 'nice': False, 'son': False, 'perfect': False, 'side': False, 'seeing': False, 'able': False, 'finds': False, 'children': True, 'days': False, 'past': False, 'summer': False, 'camera': False, 'won': False, 'including': False, 'mr': False, 'kids': True, 'lives': False, 'directed': False, 'moment': False, 'game': False, 'running': False, 'fight': False, 'supposed': False, 'video': False, 'car': False, 'matter': False, 'kevin': False, 'joe': False, 'lines': True, 'worth': False, '=': False, 'daughter': False, 'earth': False, 'starts': False, 'need': False, 'entertaining': False, 'white': False, 'start': False, 'writer': False, 'dark': False, 'short': False, 'self': False, 'worst': False, 'nearly': False, 'opening': False, 'try': True, 'upon': False, 'care': False, 'early': False, 'violence': False, 'throughout': False, 'team': True, 'production': False, 'example': False, 'beautiful': False, 'title': False, 'exactly': False, 'jack': False, 'review': False, 'major': False, 'drama': False, '&': False, 'problems': False, 'sequences': False, 'obvious': True, 'version': True, 'screenplay': False, 'known': False, 'killer': False, 'wasn': False, 'robert': False, 'disney': False, 'already': False, 'close': True, 'classic': False, 'others': False, 'hit': False, 'kill': True, 'deep': True, 'five': False, 'order': False, 'act': False, 'simple': False, 'fine': False, 'themselves': False, 'heart': False, 'roles': True, 'jackie': False, 'direction': False, 'eyes': False, 'four': False, 'question': True, 'sort': False, 'sometimes': False, 'knows': False, 'supporting': True, 'coming': False, 'voice': False, 'women': False, 'truly': False, 'save': False, 'jokes': False, 'computer': False, 'child': False, 'o': False, 'boring': False, 'tom': False, 'level': False, '1': False, 'body': True, 'guys': False, 'genre': False, 'brother': False, 'strong': False, 'stop': False, 'room': False, 'space': False, 'lee': False, 'ends': False, 'beginning': False, 'ship': False, 'york': False, 'attempt': False, 'thriller': False, 'scream': False, 'peter': False, 'aren': False, 'husband': False, 'fiction': False, 'happens': False, 'hero': False, 'novel': False, 'note': False, 'hope': False, 'king': False, 'yes': False, 'says': False, 'tells': False, 'quickly': False, 'romantic': False, 'dog': False, 'oscar': False, 'stupid': False, 'possible': False, 'saw': False, 'lead': False, 'career': False, 'murder': True, 'extremely': True, 'manages': False, 'god': False, 'mostly': False, 'wonder': True, 'particularly': False, 'future': False, 'fans': False, 'sound': False, 'worse': False, 'piece': False, 'involving': False, 'de': False, 'appears': False, 'planet': False, 'paul': False, 'involved': False, 'mean': False, 'none': False, 'taking': False, 'hours': False, 'laugh': False, 'police': False, 'sets': False, 'attention': True, 'co': False, 'hell': False, 'eventually': False, 'single': False, 'fall': False, 'falls': False, 'material': False, 'emotional': False, 'power': False, 'late': False, 'lack': False, 'dr': False, 'van': False, 'result': False, 'elements': False, 'meet': False, 'smith': False, 'science': False, 'experience': False, 'bring': False, 'wild': False, 'living': False, 'theater': False, 'interest': False, 'leads': False, 'word': False, 'feature': False, 'battle': False, 'girls': False, 'alone': False, 'obviously': False, 'george': False, 'within': True, 'usually': False, 'enjoy': False, 'guess': False, 'among': False, 'taken': False, 'feeling': False, 'laughs': False, 'aliens': False, 'talk': False, 'chance': False, 'talent': False, '3': False, 'middle': False, 'number': False, 'easy': False, 'across': False, 'needs': False, 'attempts': True, 'happen': True, 'television': False, 'chris': False, 'deal': False, 'poor': False, 'form': False, 'girlfriend': False, 'viewer': True, 'release': False, 'killed': False, 'forced': False, 'whether': False, 'wonderful': False, 'feels': False, 'oh': False, 'tale': False, 'serious': False, 'expect': False, 'except': False, 'light': False, 'success': False, 'features': False, 'premise': False, 'happy': False, 'words': False, 'leave': False, 'important': False, 'meets': False, 'history': True, 'giving': False, 'crew': False, 'type': False, 'call': False, 'turned': False, 'released': False, 'parents': False, 'art': False, 'impressive': False, 'mission': False, 'working': False, 'seemed': False, 'score': False, 'told': False, 'recent': False, 'robin': False, 'basically': False, 'entertainment': False, 'america': False, '$': False, 'surprise': False, 'apparently': False, 'easily': False, 'ryan': False, 'cool': False, 'stuff': False, 'cop': False, 'change': False, 'williams': False, 'crime': True, 'office': False, 'parts': False, 'somehow': False, 'sequel': False, 'william': False, 'cut': False, 'die': False, 'jones': False, 'credits': False, 'batman': False, 'suspense': False, 'brings': False, 'events': False, 'reality': False, 'whom': False, 'local': False, 'talking': False, 'difficult': False, 'using': False, 'went': False, 'writing': False, 'remember': False, 'near': False, 'straight': False, 'hilarious': False, 'ago': False, 'certain': True, 'ben': False, 'kid': False, 'wouldn': False, 'slow': False, 'blood': True, 'mystery': False, 'complete': False, 'red': False, 'popular': False, 'effective': False, 'am': False, 'fast': False, 'flick': False, 'due': True, 'runs': False, 'gone': False, 'return': False, 'presence': False, 'quality': False, 'dramatic': True, 'filmmakers': False, 'age': False, 'brothers': False, 'business': False, 'general': False, 'rock': False, 'sexual': False, 'present': False, 'surprisingly': False, 'anyway': False, 'uses': False, '4': False, 'personal': False, 'figure': False, 'smart': False, 'ways': False, 'decides': False, 'annoying': False, 'begin': False, 'couldn': False, 'somewhat': False, 'shots': False, 'rich': False, 'minute': False, 'law': True, 'previous': False, 'jim': False, 'successful': False, 'harry': False, 'water': False, 'similar': False, 'absolutely': False, 'motion': False, 'former': False, 'strange': False, 'came': False, 'follow': True, 'read': False, 'project': False, 'million': False, 'secret': False, 'starring': False, 'clear': False, 'familiar': False, 'romance': False, 'intelligent': True, 'third': False, 'excellent': False, 'amazing': False, 'party': False, 'budget': False, 'eye': False, 'actress': False, 'prison': False, 'latest': False, 'means': False, 'company': False, 'towards': False, 'predictable': False, 'powerful': False, 'nor': False, 'bob': False, 'beyond': False, 'visual': False, 'leaves': False, 'r': False, 'nature': False, 'following': False, 'villain': False, 'leaving': False, 'animated': False, 'low': False, 'myself': False, 'b': False, 'bill': False, 'sam': False, 'filled': False, 'wars': False, 'questions': False, 'cinema': False, 'message': False, 'box': False, 'moving': False, 'herself': False, 'country': False, 'usual': False, 'martin': False, 'definitely': False, 'add': False, 'large': False, 'clever': False, 'create': False, 'felt': False, 'stories': False, 'brilliant': False, 'ones': False, 'giant': False, 'situation': True, 'murphy': False, 'break': False, 'opens': True, 'scary': False, 'doubt': True, 'drug': False, 'bunch': False, 'thinking': False, 'solid': True, 'effect': False, 'learn': False, 'move': False, 'force': False, 'potential': False, 'seriously': False, 'follows': False, 'above': False, 'saying': False, 'huge': False, 'class': False, 'plan': False, 'agent': False, 'created': False, 'unlike': False, 'pay': False, 'non': False, 'married': False, 'mark': False, 'sweet': False, 'perfectly': False, 'ex': False, 'realize': False, 'audiences': False, 'took': False, 'decent': False, 'likely': False, 'dream': False, 'view': False, 'scott': False, 'subject': False, 'understand': False, 'happened': True, 'enjoyable': False, 'studio': False, 'immediately': True, 'open': False, 'e': False, 'points': False, 'heard': False, 'viewers': False, 'cameron': False, 'truman': False, 'bruce': False, 'frank': False, 'private': False, 'stay': False, 'fails': False, 'impossible': False, 'cold': True, 'richard': False, 'overall': False, 'merely': False, 'exciting': False, 'mess': False, 'chase': False, 'free': False, 'ten': False, 'neither': False, 'wanted': False, 'gun': False, 'appear': False, 'carter': False, 'escape': False, 'ultimately': True, '+': False, 'fan': False, 'inside': False, 'favorite': False, 'haven': False, 'modern': False, 'l': False, 'wedding': False, 'stone': False, 'trek': False, 'brought': False, 'trouble': False, 'otherwise': False, 'tim': False, '5': False, 'allen': False, 'bond': False, 'society': False, 'liked': False, 'dumb': False, 'musical': False, 'stand': True, 'political': False, 'various': False, 'talented': False, 'particular': False, 'west': False, 'state': False, 'keeps': False, 'english': False, 'silly': False, 'u': False, 'situations': False, 'park': False, 'teen': False, 'rating': False, 'slightly': False, 'steve': False, 'truth': False, 'air': False, 'element': False, 'joke': False, 'spend': False, 'key': False, 'biggest': False, 'members': False, 'effort': False, 'government': False, 'focus': False, 'eddie': False, 'soundtrack': False, 'hands': False, 'earlier': False, 'chan': False, 'purpose': False, 'today': False, 'showing': False, 'memorable': False, 'six': False, 'cannot': True, 'max': False, 'offers': False, 'rated': False, 'mars': False, 'heavy': False, 'totally': False, 'control': False, 'credit': False, 'fi': False, 'woody': False, 'ideas': False, 'sci': False, 'wait': False, 'sit': False, 'female': False, 'ask': False, 'waste': False, 'terrible': False, 'depth': False, 'simon': False, 'aspect': False, 'list': False, 'mary': False, 'sister': False, 'animation': False, 'entirely': False, 'fear': False, 'steven': False, 'moves': False, 'actual': False, 'army': False, 'british': False, 'constantly': False, 'fire': False, 'convincing': False, 'setting': False, 'gave': False, 'tension': False, 'street': False, '8': False, 'brief': False, 'ridiculous': False, 'cinematography': False, 'typical': False, 'nick': False, 'screenwriter': False, 'ability': False, 'spent': False, 'quick': False, 'violent': False, 'atmosphere': False, 'subtle': False, 'expected': False, 'fairly': True, 'seven': False, 'killing': False, 'tone': False, 'master': False, 'disaster': False, 'lots': False, 'thinks': True, 'song': False, 'cheap': False, 'suddenly': True, 'background': True, 'club': False, 'willis': False, 'whatever': False, 'highly': False, 'sees': False, 'complex': False, 'greatest': False, 'impact': False, 'beauty': False, 'front': False, 'humans': False, 'indeed': False, 'flat': False, 'grace': False, 'wrote': False, 'amusing': False, 'ii': False, 'mike': False, 'further': False, 'cute': False, 'dull': False, 'minor': False, 'recently': False, 'hate': False, 'outside': False, 'plenty': False, 'wish': False, 'godzilla': False, 'college': False, 'titanic': False, 'sounds': False, 'telling': False, 'sight': False, 'double': False, 'cinematic': False, 'queen': False, 'hold': False, 'meanwhile': False, 'awful': False, 'clearly': False, 'theme': False, 'hear': False, 'x': False, 'amount': True, 'baby': False, 'approach': False, 'dreams': False, 'shown': False, 'island': False, 'reasons': False, 'charm': False, 'miss': False, 'longer': False, 'common': False, 'sean': False, 'carry': False, 'believable': False, 'realistic': False, 'chemistry': False, 'possibly': False, 'casting': False, 'carrey': False, 'french': False, 'trailer': False, 'tough': False, 'produced': False, 'imagine': False, 'choice': False, 'ride': False, 'somewhere': False, 'hot': False, 'race': False, 'road': False, 'leader': False, 'thin': False, 'jerry': False, 'slowly': False, 'delivers': False, 'detective': False, 'brown': False, 'jackson': False, 'member': False, 'provide': False, 'president': False, 'puts': False, 'asks': True, 'critics': False, 'appearance': False, 'famous': False, 'okay': False, 'intelligence': False, 'energy': False, 'sent': False, 'spielberg': False, 'development': False, 'etc': False, 'language': False, 'blue': False, 'proves': False, 'vampire': False, 'seemingly': False, 'basic': False, 'caught': False, 'decide': False, 'opportunity': False, 'incredibly': False, 'images': False, 'band': False, 'j': False, 'writers': False, 'knew': False, 'interested': False, 'considering': False, 'boys': False, 'thanks': False, 'remains': False, 'climax': False, 'event': False, 'directing': False, 'conclusion': True, 'leading': False, 'ground': False, 'lies': False, 'forget': False, 'alive': True, 'tarzan': False, 'century': False, 'provides': False, 'trip': False, 'partner': False, 'central': False, 'tarantino': False, 'period': False, 'pace': False, 'yourself': False, 'worked': False, 'ready': False, 'date': False, 'thus': False, '1998': False, 'terrific': False, 'write': False, 'average': False, 'onto': False, 'songs': False, 'occasionally': False, 'doctor': False, 'stands': False, 'hardly': False, 'monster': True, 'led': False, 'mysterious': False, 'details': False, 'wasted': False, 'apart': False, 'aside': False, 'store': False, 'billy': False, 'boss': False, 'travolta': False, 'producer': False, 'pull': False, 'consider': True, 'pictures': False, 'becoming': False, 'cage': False, 'loud': False, 'looked': False, 'officer': False, 'twenty': False, 'system': False, 'contains': False, 'julia': False, 'subplot': False, 'missing': False, 'personality': False, 'building': False, 'learns': False, 'hong': False, 'la': False, 'apartment': False, '7': False, 'bizarre': False, 'powers': False, 'flaws': True, 'catch': False, 'lawyer': True, 'shoot': False, 'student': False, 'unique': False, '000': False, 'admit': False, 'concept': False, 'needed': False, 'thrown': False, 'christopher': False, 'laughing': False, 'green': False, 'twists': False, 'matthew': False, 'touch': False, 'waiting': False, 'victim': False, 'cover': False, 'machine': False, 'danny': False, 'mention': False, 'search': False, '1997': False, 'win': True, 'door': False, 'manner': False, 'train': False, 'saving': False, 'share': False, 'image': False, 'discovers': False, 'normal': True, 'cross': False, 'fox': False, 'returns': False, 'adult': False, 'adds': False, 'answer': True, 'adventure': False, 'lame': False, 'male': False, 'odd': False, 'singer': False, 'deserves': False, 'gore': False, 'states': False, 'include': False, 'equally': False, 'months': False, 'barely': False, 'directors': False, 'introduced': False, 'fashion': False, 'social': False, '1999': False, 'news': False, 'hair': False, 'dance': False, 'innocent': True, 'camp': False, 'teacher': False, 'became': False, 'sad': False, 'witch': False, 'includes': False, 'nights': False, 'jason': False, 'julie': False, 'latter': False, 'food': False, 'jennifer': False, 'land': False, 'menace': False, 'rate': False, 'storyline': False, 'contact': False, 'jean': False, 'elizabeth': False, 'fellow': False, 'changes': False, 'henry': False, 'hill': False, 'pulp': False, 'gay': False, 'tried': False, 'surprised': False, 'literally': False, 'walk': False, 'standard': False, '90': False, 'forward': False, 'wise': True, 'enjoyed': False, 'discover': False, 'pop': False, 'anderson': False, 'offer': False, 'recommend': False, 'public': False, 'drive': False, 'c': False, 'toy': False, 'charming': False, 'fair': False, 'chinese': False, 'rescue': False, 'terms': False, 'mouth': False, 'lucas': False, 'accident': False, 'dies': False, 'decided': False, 'edge': False, 'footage': False, 'culture': False, 'weak': False, 'presented': False, 'blade': False, 'younger': False, 'douglas': False, 'natural': False, 'born': False, 'generally': False, 'teenage': False, 'older': False, 'horrible': False, 'addition': False, 'sadly': False, 'creates': False, 'disturbing': False, 'roger': False, 'detail': False, 'devil': False, 'debut': False, 'track': False, 'developed': False, 'week': False, 'russell': False, 'attack': False, 'explain': False, 'rarely': False, 'fully': False, 'prove': False, 'exception': False, 'jeff': False, 'twist': False, 'gang': False, 'winning': False, 'jr': False, 'species': False, 'issues': False, 'fresh': False, 'rules': False, 'meaning': False, 'inspired': False, 'heroes': False, 'desperate': False, 'fighting': False, 'filmed': False, 'faces': False, 'alan': True, 'bright': False, 'ass': False, 'flying': False, 'kong': False, 'rush': False, 'forces': False, 'charles': False, 'numerous': False, 'emotions': False, 'involves': False, 'patrick': False, 'weird': False, 'apparent': False, 'information': False, 'revenge': False, 'jay': False, 'toward': False, 'surprising': False, 'twice': False, 'editing': False, 'calls': False, 'lose': False, 'vegas': False, 'stage': False, 'intended': False, 'gags': False, 'opinion': False, 'likes': False, 'crazy': False, 'owner': False, 'places': False, 'pair': False, 'genuine': False, 'epic': False, 'speak': False, 'throw': False, 'appeal': False, 'gibson': False, 'captain': False, 'military': False, '20': False, 'blair': False, 'nowhere': False, 'length': False, 'nicely': False, 'cause': False, 'pass': False, 'episode': False, 'kiss': False, 'arnold': False, 'please': False, 'hasn': True, 'phone': False, 'filmmaking': False, 'formula': False, 'boyfriend': False, 'talents': False, 'creating': False, 'kelly': False, 'buy': False, 'wide': False, 'fantasy': False, 'mood': False, 'heads': False, 'pathetic': True, 'lacks': False, 'loved': False, 'asked': False, 'mrs': False, 'witty': False, 'shakespeare': False, 'mulan': False, 'generation': False, 'affair': False, 'pieces': False, 'task': False, 'rare': False, 'kept': False, 'cameo': False, 'fascinating': False, 'ed': False, 'fbi': False, 'burton': False, 'incredible': False, 'accent': False, 'artist': False, 'superior': False, 'academy': False, 'thomas': False, 'spirit': False, 'technical': False, 'confusing': False, 'poorly': False, 'target': False, 'lover': False, 'woo': False, 'mentioned': False, 'theaters': False, 'plane': False, 'confused': False, 'dennis': False, 'rob': False, 'appropriate': False, 'christmas': True, 'considered': False, 'legend': False, 'shame': False, 'soul': False, 'matt': False, 'campbell': False, 'process': True, 'bottom': False, 'sitting': False, 'brain': False, 'creepy': False, '13': False, 'forever': False, 'dude': False, 'crap': False, 'superb': False, 'speech': False, 'ice': False, 'journey': False, 'masterpiece': False, 'intriguing': False, 'names': False, 'pick': False, 'speaking': False, 'virtually': False, 'award': False, 'worthy': False, 'marriage': False, 'deliver': False, 'cash': False, 'magic': False, 'respect': False, 'product': False, 'necessary': False, 'suppose': False, 'silent': False, 'pointless': False, 'station': False, 'affleck': False, 'dimensional': False, 'charlie': False, 'allows': False, 'avoid': False, 'meant': False, 'cops': False, 'attitude': False, 'relationships': False, 'hits': False, 'stephen': False, 'spends': False, 'relief': False, 'physical': False, 'count': False, 'reviews': False, 'appreciate': False, 'cliches': False, 'holds': False, 'pure': False, 'plans': False, 'limited': False, 'failed': True, 'pain': False, 'impression': False, 'unless': False, 'sub': False, '[': False, 'total': False, 'creature': False, 'viewing': False, 'loves': False, 'princess': False, 'kate': False, 'rising': False, 'woods': False, 'baldwin': False, 'angry': False, 'drawn': False, 'step': False, 'matrix': False, 'themes': False, 'satire': False, 'arts': False, ']': False, 'remake': False, 'wall': False, 'moral': True, 'color': False, 'ray': False, 'stuck': False, 'touching': False, 'wit': False, 'tony': False, 'hanks': False, 'continues': False, 'damn': False, 'nobody': False, 'cartoon': False, 'keeping': False, 'realized': False, 'criminal': False, 'unfunny': False, 'comedic': False, 'martial': False, 'disappointing': False, 'anti': True, 'graphic': False, 'stunning': False, 'actions': False, 'floor': False, 'emotion': False, 'soldiers': False, 'edward': False, 'comedies': False, 'driver': False, 'expectations': False, 'added': False, 'mad': False, 'angels': False, 'shallow': False, 'suspect': False, 'humorous': False, 'phantom': False, 'appealing': False, 'device': False, 'design': False, 'industry': False, 'reach': False, 'fat': False, 'blame': True, 'united': False, 'sign': False, 'portrayal': False, 'rocky': False, 'finale': False, 'grand': False, 'opposite': False, 'hotel': False, 'match': False, 'damme': False, 'speed': False, 'ok': False, 'loving': False, 'field': False, 'larry': False, 'urban': False, 'troopers': False, 'compared': False, 'apes': False, 'rose': False, 'falling': False, 'era': False, 'loses': False, 'adults': False, 'managed': False, 'dad': False, 'therefore': False, 'pg': False, 'results': False, 'guns': False, 'radio': False, 'lady': False, 'manage': False, 'spice': False, 'naked': False, 'started': False, 'intense': False, 'humanity': False, 'wonderfully': True, 'slasher': False, 'bland': False, 'imagination': False, 'walking': False, 'willing': False, 'horse': False, 'rent': False, 'mix': False, 'generated': False, 'g': False, 'utterly': False, 'scientist': False, 'washington': False, 'notice': False, 'players': False, 'teenagers': False, 'moore': False, 'board': False, 'price': False, 'frightening': False, 'tommy': False, 'spectacular': False, 'bored': False, 'jane': False, 'join': True, 'producers': False, 'johnny': False, 'zero': False, 'vampires': False, 'adaptation': False, 'dollars': False, 'parody': False, 'documentary': False, 'dvd': False, 'wayne': False, 'post': False, 'exist': False, 'matters': False, 'chosen': False, 'mel': False, 'attractive': False, 'plain': False, 'trust': False, 'safe': False, 'reading': False, 'hoping': False, 'protagonist': False, 'feelings': False, 'fate': False, 'finding': False, 'feet': False, 'visuals': False, 'spawn': False, 'compelling': True, 'hall': False, 'sympathetic': False, 'featuring': False, 'difference': False, 'professional': False, 'drugs': False, 'ford': False, 'shooting': False, 'gold': False, 'patch': False, 'build': False, 'boat': False, 'cruise': False, 'honest': True, 'media': False, 'flicks': False, 'bug': False, 'bringing': False, 'dangerous': False, 'watched': False, 'grant': False, 'smile': False, 'plus': False, 'shouldn': False, 'decision': False, 'visually': False, 'allow': False, 'starship': False, 'roberts': False, 'dying': False, 'portrayed': False, 'turning': False, 'believes': False, 'changed': False, 'shock': False, 'destroy': False, '30': False, 'crowd': False, 'broken': False, 'tired': False, 'fail': False, 'south': False, 'died': False, 'cult': False, 'fake': False, 'vincent': False, 'identity': False, 'sexy': False, 'hunt': False, 'jedi': False, 'flynt': False, 'alex': False, 'engaging': False, 'serve': False, 'snake': False, 'yeah': False, 'expecting': False, '100': False, 'decade': False, 'ups': False, 'constant': False, 'current': False, 'survive': False, 'jimmy': False, 'buddy': False, 'send': False, 'brooks': False, 'goofy': False, 'likable': False, 'humour': False, 'technology': False, 'files': False, 'babe': False, 'aspects': False, 'presents': False, 'kills': False, 'supposedly': False, 'eight': False, 'sandler': False, 'hospital': True, 'test': False, 'hidden': False, 'brian': False, 'books': False, 'promise': False, 'determined': False, 'professor': True, 'welcome': False, 'pleasure': False, 'succeeds': False, 'individual': False, 'annie': False, 'mob': False, 'ted': False, 'virus': False, 'content': False, 'gary': False, 'direct': False, 'contrived': False, 'carpenter': False, 'scale': False, 'sick': False, 'nasty': False, 'conflict': False, 'haunting': True, 'ghost': False, 'filmmaker': False, 'japanese': False, 'helps': False, 'fare': False, 'lucky': False, 'ultimate': False, 'window': False, 'support': True, 'goal': False, 'provided': False, 'genius': True, 'winner': False, 'taylor': False, 'fantastic': False, 'faith': False, 'lynch': False, 'fit': False, 'catherine': False, 'ms': False, 'paced': False, 'breaks': False, 'al': False, 'frame': False, 'travel': False, 'badly': False, 'available': False, 'cares': False, 'reeves': False, 'crash': False, 'driving': False, 'press': False, 'seagal': False, 'amy': False, '9': False, 'headed': False, 'instance': False, 'excuse': False, 'offensive': False, 'narrative': True, 'fault': False, 'bus': False, 'f': False, 'extreme': False, 'miller': False, 'guilty': False, 'grows': False, 'overly': True, 'liners': False, 'forgotten': False, 'ahead': False, 'accept': False, 'porn': False, 'directly': False, 'helen': False, 'began': False, 'lord': False, 'folks': False, 'mediocre': False, 'bar': False, 'surface': False, 'super': False, 'failure': False, '6': False, 'acted': False, 'quiet': False, 'laughable': False, 'sheer': False, 'security': False, 'emotionally': False, 'season': False, 'stuart': False, 'jail': False, 'deals': False, 'cheesy': False, 'court': False, 'beach': False, 'austin': False, 'model': False, 'outstanding': False, 'substance': False, 'nudity': False, 'slapstick': False, 'joan': False, 'reveal': False, 'placed': False, 'check': False, 'beast': False, 'hurt': False, 'bloody': False, 'acts': False, 'fame': False, 'meeting': False, 'nuclear': False, '1996': False, 'strength': False, 'center': False, 'funniest': False, 'standing': True, 'damon': False, 'clich': False, 'position': False, 'desire': False, 'driven': False, 'seat': False, 'stock': False, 'wondering': False, 'realizes': False, 'dealing': False, 'taste': False, 'routine': False, 'comparison': False, 'cinematographer': False, 'seconds': False, 'singing': False, 'gangster': False, 'responsible': True, 'football': False, 'remarkable': False, 'hunting': False, 'adams': False, 'fly': False, 'suspects': False, 'treat': False, 'hopes': False, 'heaven': False, 'myers': False, 'struggle': False, 'costumes': False, 'beat': False, 'happening': False, 'skills': False, 'ugly': False, 'figures': False, 'thoroughly': False, 'ill': False, 'surprises': False, 'player': False, 'rival': False, 'guard': False, 'anthony': False, 'strike': False, 'community': False, 'streets': False, 'hopkins': False, 'ended': False, 'originally': False, 'sarah': False, 'creative': False, 'characterization': False, 'thankfully': False, 'growing': False, 'sharp': False, 'williamson': False, 'eccentric': False, 'explained': False, 'hey': False, 'claire': False, 'steal': False, 'inevitable': False, 'joel': False, 'core': False, 'weren': False, 'sorry': False, 'built': False, 'anne': False, 'breaking': False, 'villains': False, 'critic': False, 'lets': False, 'visit': False, 'followed': False}\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets[0][0])) #첫째 feature set의 첫째 element 즉 bag_of_words feature의 수 - 상위 2000개 단어\n",
    "print(featuresets[0][1]) #첫째 feature set의 둘째 element 즉 label\n",
    "print(featuresets[0][0]) # 첫째 feature set의 내용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW feature set을 이용한 분류\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.3 : 1.0\n",
      "                   mulan = True              pos : neg    =      9.1 : 1.0\n",
      "                  seagal = True              neg : pos    =      8.1 : 1.0\n",
      "                   damon = True              pos : neg    =      7.9 : 1.0\n",
      "             wonderfully = True              pos : neg    =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100] #train set과 test set으로 분리\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # train set으로 학습\n",
    "print(nltk.classify.accuracy(classifier, test_set)) # test set으로 분류기 성능을 평가\n",
    "classifier.show_most_informative_features(5) #분류기에서 가장 중요한 영향을 미치는 단어 상위 5개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "pos\n",
      "neg\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "testfeat = bag_of_words(['the', 'actor', 'was', 'seagal']) \n",
    "print(classifier.classify(testfeat))\n",
    "testfeat = bag_of_words(['the', 'story', 'was', 'outstanding'])\n",
    "print(classifier.classify(testfeat))\n",
    "negfeat = bag_of_words(['the', 'plot', 'was', 'ludicrous']) # 터무니없는\n",
    "print(classifier.classify(negfeat))\n",
    "posfeat = bag_of_words(['kate', 'winslet', 'is', 'accessible']) # 이해가 돼!\n",
    "print(classifier.classify(posfeat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vector with Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 준비, 입력이 raw text임\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary=[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is',\n",
      "                            'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as',\n",
      "                            'with', 'for', 'his', 'this', 'film', 'i', 'he',\n",
      "                            'but', 'on', 'are', 't', 'by', ...])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv = CountVectorizer()\n",
    "cv = CountVectorizer(vocabulary=word_features) #빈도수 상위 2,000개의 단어만 사용하여 count vector 객체를 생성\n",
    "print(cv) #객체 parameter들을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews count: 2000\n"
     ]
    }
   ],
   "source": [
    "print('reviews count:', len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', '?', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', ':', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', ';', 'first', '--', 'after', 'see', '!', 'way', 'because', 'make', 'life']\n"
     ]
    }
   ],
   "source": [
    "X = cv.fit_transform(reviews) #review를 이용하여 count vector를 학습하고, 변환\n",
    "print(cv.get_feature_names()[:100]) # count vector에 사용된 feature 이름을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 38  0  0 20 16 16  0 12  8  0  0 25 13  0  0  0  1  5  4  1 10  6  0\n",
      "  1 10  4 13  0  2  1  3  6  3  3  3  3  4  0  0  2  5  3  4  6  0 10  3\n",
      "  3  3  2  2  2  4  1  4  2  0  0  0  0  4  0  0  4  1  5  3  1  0  1  2\n",
      "  0  4  4  0  0  1  2  0  2  1  3  0  2  2  1  0  2  1  0  0  0  2  2  0\n",
      "  3  2  5  1]\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(X[0].toarray()[0, :100]) #변환된 결과의 첫째 feature set 중에서 앞 100개를 출력\n",
    "print(max(X[0].toarray()[0])) #변환된 결과의 첫째 feature set 중에서 max 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", : 0\n",
      "the : 38\n",
      ". : 0\n",
      "a : 0\n",
      "and : 20\n",
      "of : 16\n",
      "to : 16\n",
      "' : 0\n",
      "is : 12\n",
      "in : 8\n",
      "s : 0\n",
      "\" : 0\n",
      "it : 25\n",
      "that : 13\n",
      "- : 0\n",
      ") : 0\n",
      "( : 0\n",
      "as : 1\n",
      "with : 5\n",
      "for : 4\n"
     ]
    }
   ],
   "source": [
    "for word, count in zip(cv.get_feature_names()[:20], X[0].toarray()[0, :20]):\n",
    "    print(word, ':', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 간 유사도 계산 및 가장 유사한 문서 검색\n",
    "### Cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(0,2*np.pi,0.1)   # start,stop,step\n",
    "y = np.cos(x)\n",
    "#print(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "new_review = \"first of all it's a plot heavy mess that has bad voice talents , badly written script and fantastic animation. they are small pokemon with a powerful punch and have great psychic abilities\"\n",
    "new_vec = cv.transform([new_review]) #문서를 count vector로 변환\n",
    "\n",
    "sim_result = cosine_similarity(new_vec, X) #변환된 count vector와 기존 값들과의 similarity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47404152357849716, 0.4354705693078365, 0.4179484687443539, 0.40669013197195214, 0.4042441311205991, 0.4031257527594671, 0.39316730570210306, 0.38744458895341755, 0.3852303002224743, 0.384468719848977]\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/howto/sorting.html\n",
    "print(sorted(sim_result[0], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(sim_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \" pokemon 3 : the movie \" has a lot of bad things in it . \n",
      "first of all it's a plot heavy mess that has bad voice talents , badly written script and fantastic animation . \n",
      "the first film came out the end of 1999 and was a huge hit grossing almost $90 million domestically . \n",
      "a sequel soon followed and even made $45 million . \n",
      "warner has released their third movie based on the immensely popular video game and tv series and its a waste of time and celluloid . \n",
      "this time ash ketchum and his friends are on their way to the johto battles ( which my little brother told me the new spinoff is \" pokemon : the johto journeys \" so go figure ) anyway he comes in contact with a young girl who's father has disappeared after trying to discover the unown . \n",
      "they are small pokemon with a powerful punch and have great psychic abilities . \n",
      "the unown bring together their psychic abilities and create entei a powerful legendary pokemon who barriers young molly's house and creates every wish she wants . \n",
      "now it's up to ash and his friends to stop this pokemon entei and show him to be a good pokemon rather than a bad one . \n",
      "too bad really that this is a bad movie , surprisingly the first movie was entertaining and somewhat absorbing , the second was a piece of trash and this one is almost in between . \n",
      "it has some good qualities ( animation , message in the end ) but the flaws seem to overpower the goods . \n",
      "i'm still not sure what the big thing is about pokemon , they are ugly little animals who speak their own name for their language ( besides meowth , my personal favorite ) and you don't understand what they are saying . \n",
      "my little brother just thought the movie was amazing , and i kept leaning over and asking him happened , or what pokemon that was . \n",
      "his response was a big lecture of how this is that , and that is this . . . he \n",
      "sure did put me in my place . \n",
      "with the second and third movie being bad , i have a feeling pokemon 4 : the movie might be a total bust as well . \n",
      " \" pokemon 3 : the movie \" has some redeeming qualities for the kids , and the pokemon fans will dig every minute of this film . \n",
      "for those parents and/or brothers and sisters who have to sit through this . . . bring a pillow . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reviews[679])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 679,  176, 1152, 1575,  952,  470,  688, 1144,  103,  159],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-sim_result[0]).argsort()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF with Scikit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n",
      "max count score of the first vector: 38\n",
      "max tfidf score of the first vector: 0.3958279594831942\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = transformer.fit_transform(X)\n",
    "print(X_tfidf.shape)\n",
    "print('max count score of the first vector:', max(X[0].toarray()[0]))\n",
    "print('max tfidf score of the first vector:', max(X_tfidf[0].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tfidf = transformer.transform(new_vec)\n",
    "sim_result_tf = cosine_similarity(new_tfidf, X_tfidf)\n",
    "np.argmax(sim_result_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39386919931415737, 0.2158533727955655, 0.2055888622850777, 0.18940610744482467, 0.18862790397285573, 0.18841943811025338, 0.18329868368380425, 0.1810784645715442, 0.17853711709135542, 0.17620991687411133]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(sim_result_tf[0], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vector: [ 679  176 1152 1575  952  470  688 1144  103  159]\n",
      "TFIDF vector: [ 679  577 1209 1933  672 1280    3  913 1596 1163]\n"
     ]
    }
   ],
   "source": [
    "#count vector에 대한 유사도 상위 문서와 tfidf에 대한 유사도 상위 문서를 비교\n",
    "print('Count vector:', (-sim_result[0]).argsort()[:10])\n",
    "print('TFIDF vector:', (-sim_result_tf[0]).argsort()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9245\n",
      "0.9645\n",
      "0.9135\n",
      "0.854\n",
      "0.9365\n",
      "0.809\n",
      "0.809\n",
      "0.9165\n",
      "0.883\n",
      "0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "tfidf_list = [679, 1280,  577, 1933, 1209, 1115, 1274, 913, 1456, 1796]\n",
    "for i in tfidf_list:\n",
    "    print(jaccard_similarity_score(new_vec.toarray()[0], X[i].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension을 줄이는 방법: SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01514489 0.0166241  0.01317354 0.01030468 0.00756888 0.00725983\n",
      " 0.00659312 0.00580567 0.00550706 0.00504246 0.00483872 0.00470389\n",
      " 0.00453713 0.0043536  0.00431135 0.00415999 0.004104   0.00402205\n",
      " 0.00382987 0.00380754 0.0036883  0.00367408 0.00363124 0.00352173\n",
      " 0.00347459 0.00339472 0.00335051 0.00325916 0.00323815 0.00315384\n",
      " 0.00314674 0.00304913 0.00300347 0.00299865 0.00295145 0.00292693\n",
      " 0.00289485 0.00287923 0.00282802 0.00280223 0.00277773 0.00274264\n",
      " 0.00271311 0.00268979 0.00265569 0.00264501 0.00259901 0.00257714\n",
      " 0.0025622  0.00254445 0.00249797 0.00245484 0.00243712 0.00241656\n",
      " 0.00238654 0.00235366 0.00234395 0.00233282 0.00229758 0.002295\n",
      " 0.00227465 0.00226673 0.00224546 0.00223066 0.00221365 0.00220156\n",
      " 0.00219036 0.00218628 0.00216656 0.00216035 0.00215562 0.00211927\n",
      " 0.00211044 0.00207907 0.00207117 0.00205964 0.00205848 0.00203116\n",
      " 0.00202441 0.00199946 0.00199753 0.00199207 0.00197294 0.0019617\n",
      " 0.0019585  0.00193714 0.00191767 0.00191458 0.00189811 0.00186771\n",
      " 0.00185353 0.00184663 0.00183936 0.00182079 0.00180437 0.00178908\n",
      " 0.00177404 0.00176876 0.00174958 0.00173811]\n",
      "0.3301294081280183\n",
      "[29.54936635  4.36420045  3.88223926  3.4336061   2.9432501   2.88264756\n",
      "  2.74664301  2.57756775  2.51054801  2.40189889  2.35317037  2.3198506\n",
      "  2.27889108  2.23179972  2.22167603  2.1816127   2.16713559  2.14527806\n",
      "  2.0932693   2.0872057   2.05431727  2.05033882  2.03837721  2.0072858\n",
      "  1.99390129  1.97095962  1.95792141  1.93100751  1.92478453  1.89965894\n",
      "  1.89741067  1.86775163  1.85373119  1.85222777  1.83759385  1.82997386\n",
      "  1.81990837  1.81497947  1.79875771  1.79060839  1.78279738  1.77145545\n",
      "  1.76188644  1.75434521  1.74313166  1.73962107  1.72447955  1.71713989\n",
      "  1.71222068  1.70619451  1.69053883  1.67588603  1.66985071  1.66281531\n",
      "  1.65241505  1.641018    1.6376141   1.633811    1.62131504  1.62043129\n",
      "  1.61322709  1.61039551  1.60282223  1.59752576  1.59172378  1.58707378\n",
      "  1.58303285  1.58159752  1.57460225  1.57221768  1.57049958  1.55739933\n",
      "  1.55388822  1.54230728  1.5394555   1.53506889  1.5346513   1.52441458\n",
      "  1.52188451  1.51253005  1.51175704  1.50967713  1.50242339  1.49812574\n",
      "  1.49690923  1.48872122  1.48122565  1.48002615  1.47364228  1.46183106\n",
      "  1.45625308  1.45355162  1.45067424  1.44333001  1.43679407  1.43069132\n",
      "  1.42471488  1.42255847  1.41482775  1.41022141]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_tfidf)  \n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "print(svd.singular_values_)  \n",
    "newX = svd.transform(X_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2000)\n",
      "(2000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(svd.components_.shape)\n",
    "print(newX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.36921139e-28,  6.30071943e-01,  4.74953485e-31, ...,\n",
       "        2.65980872e-03,  2.57052959e-03,  2.46302852e-03])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews count: 2000\n",
      "Length of the first review: 4043\n",
      "Labels: {'neg', 'pos'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] \n",
    "#file id를 이용해 label로 사용할 category 즉 positive와 negative 정보를 순서대로 가져옴\n",
    "\n",
    "print('Reviews count:', len(reviews))\n",
    "print('Length of the first review:', len(reviews[0]))\n",
    "print('Labels:', set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=10)\n",
    "# sklearn의 train_test_split 함수는 먼저 data set을 shuffle하고 주어진 비율에 따라 train set과 test set을 나눠 줌\n",
    "# 위에서는 reviews를 X_train과 X_test로 8:2의 비율로 나누고, categories를 y_train과 y_test로 나눔\n",
    "# 이 때 X와 y의 순서는 동일하게 유지해서 각 입력값과 label이 정확하게 match되도록 함\n",
    "# random_state는 shuffle에서의 seed 값으로, 지정한 경우 항상 동일한 결과로 shuffle이 됨\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#sklearn에서 제공하는 TfidfVectorizer를 이용\n",
    "tfidf = TfidfVectorizer().fit(X_train) # X_train을 이용하여 vectorizer를 학습\n",
    "tfidf #vectorize에서 사용한 매개변수 값들을 확인 -> 현재는 모두 default 값을 사용, 향후 tokenizer, max_features 등을 지정할 수 있음\n",
    "# 상세한 매개변수 내용은 위 링크를 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 36310)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train) #학습된 vectorizer를 이용하여 train set을 변환\n",
    "X_train_tfidf.shape # 1600 (review 수) x 36310 (전체 corpus에서 사용된 단어의 수) 크기로 vector set이 생성됨\n",
    "# matrix 안의 값은 해당 tfidf score임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=2000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000).fit(X_train) #사용된 단어의 수가 너무 많은 경우, max_feature를 제한하여 학습이 가능\n",
    "tfidf #max_factures 값이 사용된 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1600, 2000)\n",
      "Test set dimension: (400, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_tfidf.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_tfidf = tfidf.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayse Classifier (Scikit사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1600, 2000)\n",
      "Test set dimension: (400, 2000)\n"
     ]
    }
   ],
   "source": [
    "#나이브 베이즈는 word count를 사용하므로 tfdif가 아닌 count vectorizer를 사용하여 학습 및 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000).fit(X_train) #tfidf와 동일하게 max_feature를 제한하여 학습\n",
    "X_train_cv = cv.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 를 사용\n",
    "NB_clf = MultinomialNB() # 분류기 선언\n",
    "\n",
    "NB_clf.fit(X_train_cv, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
    "#NB_clf.fit(X_train_tfidf, y_train) #tfidf 값을 사용할 수도 있으나, NB 이론에 맞지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.864\n",
      "Test set score: 0.775\n"
     ]
    }
   ],
   "source": [
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train))) #train set에 대한 예측정확도를 확인\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test))) #test set에 대한 예측정확도를 확인\n",
    "#실제로 필요한 것은 test set에 대한 예측정확도이나, 과적합 (overfitting)의 문제가 있는지를 보기 위해 train set에 대한 예측정확도를 같이 확인\n",
    "#print('Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "#print('Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg' 'pos']\n",
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "#여러 문장에 대해 count vectorier로 변환 후 학습된 분류기로 결과를 예측\n",
    "print(NB_clf.predict(cv.transform(['the story was unimaginative', 'the plot was ludicrous', 'kate winslet is accessible'])))\n",
    "\n",
    "#위 첫째 문장에서 story를 actor로 바꿔서 예측\n",
    "print(NB_clf.predict(cv.transform(['the actor was unimaginative'])))\n",
    "#동일한 형용사라도 대상에 따라 결과가 바뀔 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (Scikit사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 1.000\n",
      "Test set score: 0.830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #sklearn이 제공하는 logistic regression을 사용\n",
    "\n",
    "#count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf_cv = LogisticRegression() #분류기 선언\n",
    "LR_clf_cv.fit(X_train_cv, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(LR_clf_cv.score(X_train_cv, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(LR_clf_cv.score(X_test_cv, y_test))) # test data에 대한 예측정확도\n",
    "# count vector를 이용한 regression 결과가 tfidf보다 더 좋게 나옴\n",
    "# 보통은 tfidf가 더 좋은 결과를 보이는데, 이와 같이 상황에 따라 다른 결과가 나오기도 함\n",
    "# 지금은 train data의 수가 1,600개인데 비해, 추정해야 하는 parameter의 수가 2,000개로 sample 수가 학습에 부족한 상황, \n",
    "# 따라서 예상 못한 다양한 결과가 나올 수 있음\n",
    "# 좀더 data가 많은 상황에서의 test가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.917\n",
      "Test set score: 0.820\n"
     ]
    }
   ],
   "source": [
    "#tfidf vector를 이용해서 분류기 학습\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "LR_clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(LR_clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(LR_clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도\n",
    "# NB에 비해 더 좋은 결과가 나오는 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'neg', 'pos'], dtype='<U3')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB 분류기에서 사용했던 예제로 결과 확인, 실제로 결과가 더 나아졌음을 확인할 수 있음\n",
    "LR_clf.predict(tfidf.transform(['the story was unimaginative', 'the plot was ludicrous', 'kate winslet is accessible']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.974\n",
      "Test set score: 0.840\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "# 일반적으로 ridge regression을 쓰면 쓰지 않은 경우보다 train data에 대한 예측정확도는 떨어지고 test data는 올라가게 됨\n",
    "# 여기서는 train set에 대한 예측정확도가 같이 상승하는 진귀한 경우가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.804\n",
      "Test set score: 0.770\n",
      "Used features count: 78 out of 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
    "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) \n",
    "# parameter 혹은 coefficient 중에서 0이 아닌 것들의 개수를 출력\n",
    "# 2000개 중에서 78개만 선택된 것을 볼 수 있음\n",
    "# 예측률은 rigde나 일반 logistic에 비해 떨어지지만, 실제로 영향을 미치는 단어들이 어떤 것들인지 확인할 수 있다는 장점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['000', '10', '100', '13', '15', '1995', '1996', '1997', '1998', '1999']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tfidf.vocabulary_)) # tfidf에 사용된 단어의 수\n",
    "tfidf_voca = tfidf.get_feature_names() # tfidf에서 단어이름을 가져옴\n",
    "tfidf_voca[:10] # 앞 10개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "print((lasso_clf.coef_ != 0)[0].tolist()[:100]) \n",
    "# lasso에 사용된 단어들 중 coefficient의 사용여부를 리스트로 변환하여 앞부터 100개를 출력해 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "for i, sign in enumerate((lasso_clf.coef_ != 0)[0].tolist()):\n",
    "    if sign: selected_features.append(tfidf_voca[i]) #사용여부가 True인 단어들만 selected_features에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aliens', 'also', 'and', 'any', 'as', 'attempt', 'awful', 'bad', 'batman', 'boring', 'cameron', 'carpenter', 'could', 'definitely', 'director', 'dull', 'even', 'excellent', 'family', 'great', 'hanks', 'harry', 'have', 'he', 'her', 'here', 'his', 'in', 'is', 'jackie', 'julie', 'lame', 'least', 'lebowski', 'life', 'looks', 'many', 'matrix', 'mess', 'most', 'movie', 'mulan', 'no', 'nothing', 'off', 'on', 'only', 'perfect', 'perfectly', 'plot', 'political', 'poor', 'quite', 'reason', 'ridiculous', 'scream', 'script', 'seagal', 'see', 'seen', 'shrek', 'so', 'stupid', 'supposed', 'the', 'then', 'there', 'this', 'to', 'truman', 'unfortunately', 'very', 'war', 'waste', 'well', 'west', 'why', 'worst']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(selected_features) #78개의 선택된 단어들을 출력 - 즉 positive, negative에 결정적 영향을 미치는 단어들\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aliens'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_voca[(lasso_clf.coef_ != 0)[0].tolist().index(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aliens', 'also', 'and', 'any', 'as', 'attempt', 'awful', 'bad', 'batman', 'boring', 'cameron', 'carpenter', 'could', 'definitely', 'director', 'dull', 'even', 'excellent', 'family', 'great', 'hanks', 'harry', 'have', 'he', 'her', 'here', 'his', 'in', 'is', 'jackie', 'julie', 'lame', 'least', 'lebowski', 'life', 'looks', 'many', 'matrix', 'mess', 'most', 'movie', 'mulan', 'no', 'nothing', 'off', 'on', 'only', 'perfect', 'perfectly', 'plot', 'political', 'poor', 'quite', 'reason', 'ridiculous', 'scream', 'script', 'seagal', 'see', 'seen', 'shrek', 'so', 'stupid', 'supposed', 'the', 'then', 'there', 'this', 'to', 'truman', 'unfortunately', 'very', 'war', 'waste', 'well', 'west', 'why', 'worst']\n"
     ]
    }
   ],
   "source": [
    "print([tfidf_voca[i] for i, j in enumerate((lasso_clf.coef_ != 0)[0].tolist()) if j]) #선택된 단어들을 출력하는 또다른 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (scikit-learn 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01525261 0.01669138 0.0133583  0.01042773 0.00741403 0.00730082\n",
      " 0.00677966 0.00583257 0.00555167 0.00520324 0.00493018 0.00489498\n",
      " 0.00458197 0.0044574  0.00432849 0.00421975 0.0041921  0.00413251\n",
      " 0.00403416 0.00398954 0.00388074 0.00379435 0.00377392 0.00370049\n",
      " 0.00361687 0.00358558 0.00350955 0.00340951 0.0033149  0.00327589\n",
      " 0.00321078 0.00319169 0.00315556 0.00311055 0.00304756 0.00302892\n",
      " 0.00300949 0.00297231 0.00293611 0.00292412 0.00290208 0.00287548\n",
      " 0.0028342  0.00280742 0.00276097 0.00274277 0.00270645 0.00268513\n",
      " 0.00267427 0.00265801 0.00258256 0.00257419 0.00256683 0.00255658\n",
      " 0.00251155 0.00250159 0.00248046 0.00247207 0.00245957 0.00245289\n",
      " 0.00241614 0.0023879  0.00236025 0.00234094 0.00233893 0.00231345\n",
      " 0.00229516 0.0022807  0.00227705 0.00224271 0.00222958 0.00222203\n",
      " 0.00220312 0.00219438 0.00218776 0.0021722  0.00215608 0.00215051\n",
      " 0.00213564 0.00212784 0.00211271 0.00208695 0.00207613 0.00206077\n",
      " 0.00204613 0.00203201 0.00200931 0.00199865 0.00199714 0.00197327\n",
      " 0.00196589 0.00194951 0.00194085 0.00193257 0.00190954 0.00190278\n",
      " 0.00187657 0.0018573  0.00185606 0.00183413]\n",
      "0.3412776971149226\n",
      "[26.28071313  3.92896916  3.51203674  3.10301032  2.61748589  2.59642773\n",
      "  2.50217867  2.32098855  2.26439245  2.19203943  2.13375967  2.12626195\n",
      "  2.0574806   2.02899959  1.99946031  1.97402069  1.96747448  1.95357853\n",
      "  1.93008885  1.91932804  1.89298024  1.87190046  1.86671996  1.84849394\n",
      "  1.82766938  1.819579    1.80027893  1.77431404  1.74951541  1.7391934\n",
      "  1.72195946  1.71670307  1.70696488  1.69474953  1.67750176  1.67234783\n",
      "  1.66697967  1.65665295  1.64652842  1.64316809  1.63696717  1.62944089\n",
      "  1.61771724  1.61004634  1.59668252  1.59144373  1.58090069  1.57474131\n",
      "  1.57139547  1.56660944  1.54422075  1.54187887  1.53957662  1.53645066\n",
      "  1.52294988  1.51981377  1.5133942   1.51083776  1.50699561  1.50498852\n",
      "  1.4936319   1.48487743  1.47627146  1.47043492  1.4695741   1.461563\n",
      "  1.45597621  1.45119181  1.4500094   1.43903939  1.43480772  1.43237878\n",
      "  1.42628258  1.42346676  1.4212888   1.41623469  1.41104021  1.40916811\n",
      "  1.4042597   1.40182779  1.39671277  1.38815761  1.38456007  1.37944892\n",
      "  1.37451998  1.36984485  1.36221159  1.35849975  1.35798477  1.34983287\n",
      "  1.34729707  1.34169375  1.33869015  1.3358442   1.32784891  1.32550655\n",
      "  1.31632998  1.30959701  1.3091189   1.30136956]\n",
      "(100, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_train_tfidf)  # train data로 학습 -> unsupervised이므로 y가 필요 없음\n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "# 현재 결과에서는 100개의 선택된 component가 전체 분산의 34% 정도를 설명하고 있음\n",
    "print(svd.singular_values_) #위 그림에서 오른쪽 가운데 대각행렬의 값\n",
    "print(svd.components_.shape) # 위 그림에서 가장 오른쪽 행렬의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000)\n",
      "(1600, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_svd = svd.transform(X_train_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)\n",
    "print(X_train_tfidf.shape) #축소하기 전의 차원\n",
    "print(X_train_svd.shape) #축소된 후의 차원, 2000 -> 100개로 줄어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA를 이용한 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.838\n",
      "Test set score: 0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SVD_clf = LogisticRegression()\n",
    "SVD_clf.fit(X_train_svd, y_train) #축소된 값으로 학습\n",
    "print('Train set score: {:.3f}'.format(SVD_clf.score(X_train_svd, y_train)))\n",
    "print('Test set score: {:.3f}'.format(SVD_clf.score(X_test_svd, y_test)))\n",
    "# NB, Lasso보다 좋지만 Ridge, 일반보다는 나쁜 값\n",
    "# 상황에 따라 달라질 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 문서의 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "text = []\n",
    "y = []\n",
    "with open('movie_data.csv', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        #print(row)\n",
    "        if row: #그 줄에 내용이 있는 경우에만\n",
    "            text.append(row[0]) #영화 리뷰를 text 리스트에 추가\n",
    "            y.append(row[2]) #영화이름을 text 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\user\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.16.5)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (0.4.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2019.9.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples: 1827\n",
      "Movie titles of reivews: {'라라랜드', '인피니티 워', '코코', '신과함께', '곤지암'}\n"
     ]
    }
   ],
   "source": [
    "print('Num of samples: {}'.format(len(text)))\n",
    "print('Movie titles of reivews: {}'.format(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, y, random_state=0)\n",
    "# 비율을 지정하지 않으면 75:25로 분할됨\n",
    "\n",
    "len(X_train) #1827의 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "#from konlpy.tag import Twitter #konlpy에서 Twitter 형태소 분석기를 import\n",
    "twitter_tag = Okt()\n",
    "#twitter_tag = Twitter()\n",
    "\n",
    "print(twitter_tag.morphs(X_train[1])) #둘째 리뷰에 대해 형태소 단위로 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['혹시', '역시', '편집', '사운드', '공포', '영화', '푸시', '인상', '여기', '또']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tag.nouns(X_train[1]) #둘째 리뷰에서 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_tokenizer(text): # Twitter 형태소 분석기의 명사추출함수를 tokenizer 함수로 사용\n",
    "    return twitter_tag.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8153284671532847\n",
      "Test score 0.6717724288840262\n",
      "(1370, 1156)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=2) #Twitter 형태소분석기에서 명사만 추출하는 함수를 tokenizer로 이용\n",
    "# twit_tokenizer 대신 twitter_tag.nouns를 직접 써도 됨\n",
    "# 하나의 문서에서만 출현한 단어는 쓸모가 없으므로 제외, 즉 최소 document frequency를 2로 설정\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train) # train data 변환 -> tfidf vector\n",
    "X_test_tfidf = tfidf.transform(X_test) # test data 변환 -> tfidf vector\n",
    "\n",
    "clf = LogisticRegression() # logistic regression 분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # 분류기 학습\n",
    "print('Train score', clf.score(X_train_tfidf, y_train)) # train data 예측정확도\n",
    "print('Test score', clf.score(X_test_tfidf, y_test)) # test data 예측정확도\n",
    "print(X_train_tfidf.shape) # 총 1156개의 명사로 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['졸잼 최고',\n",
       " '내용, 음악 , 연기력  무엇하나 빠지는것이 없네요 특히 음악은 계속 찾아 듣게되요^^',\n",
       " '아맥2D로 느즈막히 관람.... 히어로가 많이나오지만, 이걸 꽤나 잘 버무려놓음. 뻔한스토리의 틀을 벗어나려 노력한점은 높은점수를 줄만함.... 블럭버스터액션, 영상미는 말이필요없음...... 후속편 기대됨!',\n",
       " '후반부터 쫄렸다.',\n",
       " '진짜. 솔직히 한국 공포영화중에 이렇게 소재별로인건 정말 오랜만인듯; 지들끼리 소리지르고 정신없이 우왕자왕 심지어 무섭지도않어 효과음만크고 진짜최악임ㅉㅉ',\n",
       " '소문난 잔치에 먹을거 없음..ㅜㅜ',\n",
       " 'good!',\n",
       " '아 점수를 줄 수가 없네  화면은 왜그리도 흔들어 데는지........ 재미도 없고 가볍기만하고 .... 최악의 재미없는 배멀미 영화',\n",
       " '영화 보면서 펑펑물었네요~ 부모님 사랑에 대해 다시한번 생각하게 했던 영화네요^ ^',\n",
       " '슬픈 스토리지만 삶을 돌아보게 하는 영화다. 죄를 지은자는 그 벌을 고스란히 받으리라. 사회 각종범죄자들 뉘우치길 바란다.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10] #test data에서 앞 10개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['인피니티 워', '라라랜드', '인피니티 워', '곤지암', '곤지암', '인피니티 워', '인피니티 워',\n",
       "       '신과함께', '코코', '신과함께'], dtype='<U6')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test_tfidf[:10]) # test data의 앞 10개에 대한 예측내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인피니티 워', '라라랜드', '인피니티 워', '곤지암', '곤지암', '인피니티 워', '인피니티 워', '곤지암', '신과함께', '신과함께']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:10]) # test data 앞 10개의 실제 영화제목"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능을 개선하기 위한 노력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['혹시', '나', '하고', '봤는데', '역시', '나다', ';;', '편집', '과', '사운드', '로', '주는', '작은', '공포', '영화', \"'\", '푸시', \"'\", '에서', '인상', '깊게', '봤던걸', '여기', '서', '또', '보네', ';;']\n"
     ]
    }
   ],
   "source": [
    "# morphs()는 명사 외에도 모든 형태소를 포함\n",
    "print(twitter_tag.morphs(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8802919708029197\n",
      "Test score 0.6433260393873085\n",
      "(1370, 2260)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twitter_tag.morphs, min_df=2) # 명사 대신 모든 형태소를 사용\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "#명사만 사용한 것에 비해 train score는 상승, test score는 하락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('혹시', 'Noun'), ('나', 'Josa'), ('하다', 'Verb'), ('보다', 'Verb'), ('역시', 'Noun'), ('나다', 'Verb'), (';;', 'Punctuation'), ('편집', 'Noun'), ('과', 'Josa'), ('사운드', 'Noun'), ('로', 'Josa'), ('주다', 'Verb'), ('작다', 'Adjective'), ('공포', 'Noun'), ('영화', 'Noun'), (\"'\", 'Punctuation'), ('푸시', 'Noun'), (\"'\", 'Punctuation'), ('에서', 'Josa'), ('인상', 'Noun'), ('깊다', 'Adjective'), ('보다', 'Verb'), ('여기', 'Noun'), ('서', 'Josa'), ('또', 'Noun'), ('보다', 'Verb'), (';;', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter_tag.pos(X_train[1], norm=True, stem=True)) #pos()는 형태소와 품사를 함께 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_tokenizer2(text): #전체를 다 사용하는 대신, 명사, 동사, 형용사를 사용\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in twitter_tag.pos(text, norm=True, stem=True):\n",
    "        if tag in target_tags:\n",
    "            result.append(word)\n",
    "#            result.append('/'.join([word, tag]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['혹시', '하다', '보다', '역시', '나다', '편집', '사운드', '주다', '작다', '공포', '영화', '푸시', '인상', '깊다', '보다', '여기', '또', '보다']\n"
     ]
    }
   ],
   "source": [
    "print(twit_tokenizer2(X_train[1])) # 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8540145985401459\n",
      "Test score 0.6892778993435449\n",
      "(1370, 1584)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer2, min_df=2) #명사, 동사, 형용사를 이용하여 tfidf 생성\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "# 현재까지 중에서 test score가 가장 뛰어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 형태소를 다 사용하고 품사를 알 수 있도록 하면?\n",
    "def twit_tokenizer3(text):\n",
    "    #target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in twitter_tag.pos(text, norm=True, stem=True):\n",
    "        result.append('/'.join([word, tag])) #단어의 품사를 구분할 수 있도록 함\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8751824817518248\n",
      "Test score 0.6695842450765864\n",
      "(1370, 2023)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer3, min_df=2)\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "#성능이 오히려 떨어지고 품사 표시 없이 전체를 다 사용한 경우에 비해 train은 떨어지고, test는 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.944\n",
      "Test set score: 0.676\n"
     ]
    }
   ],
   "source": [
    "# train score가 높으므로 ridge를 쓰면 어떨까?\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier(alpha = 1)\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "# train score가 올라가는 현상이 벌어짐\n",
    "# test score가 올라갔으나 명사, 형용사, 동사를 쓴 것보다 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.718\n",
      "Test set score: 0.643\n",
      "Used features count: 240 out of 2023\n"
     ]
    }
   ],
   "source": [
    "#lasso를 쓰면?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01102934 0.01405529 0.01144551 0.0110093  0.00918016 0.00876149\n",
      " 0.00836851 0.00795922 0.00777003 0.00737423 0.00707199 0.00665546\n",
      " 0.00659729 0.00616382 0.00588976 0.00563969 0.00546929 0.00543044\n",
      " 0.00524396 0.00523751 0.0051354  0.00507564 0.00494917 0.00478661\n",
      " 0.00466253 0.0046021  0.00455272 0.00452943 0.00432636 0.0043043\n",
      " 0.00425346 0.00416261 0.00410043 0.00401296 0.00399825 0.00396182\n",
      " 0.00392779 0.00385998 0.00373325 0.00369469 0.00365484 0.00363113\n",
      " 0.00357832 0.00354544 0.00349848 0.00346928 0.00337552 0.00334918\n",
      " 0.00332458 0.00330956 0.00323571 0.00321667 0.00315217 0.00314053\n",
      " 0.00309441 0.00307477 0.0030649  0.00301359 0.00300256 0.00296081\n",
      " 0.00293836 0.00291433 0.00288938 0.00288701 0.00286437 0.00281526\n",
      " 0.00280011 0.00276482 0.00274032 0.00271676 0.00269105 0.00266254\n",
      " 0.00264105 0.00262961 0.00260318 0.00258836 0.00257263 0.0025549\n",
      " 0.00253862 0.00252722 0.00251281 0.00248844 0.00248194 0.00245121\n",
      " 0.00244755 0.00241116 0.00239864 0.0023803  0.00236743 0.00235925\n",
      " 0.00232777 0.00231998 0.00228583 0.00225879 0.00224278 0.00222963\n",
      " 0.00222807 0.00219394 0.00218403 0.00217777 0.00216885 0.00213963\n",
      " 0.00213221 0.00211726 0.0021082  0.00208491 0.00207873 0.00206984\n",
      " 0.0020442  0.00203715 0.00203109 0.00202247 0.00200684 0.00199728\n",
      " 0.00198139 0.00197578 0.00196858 0.0019567  0.00195046 0.00193928\n",
      " 0.00192188 0.00191527 0.00190398 0.00190288 0.00189315 0.00188382\n",
      " 0.00186635 0.00185401 0.00184764 0.00184161 0.0018402  0.00182065\n",
      " 0.001813   0.00179631 0.00179194 0.00178155 0.00177619 0.00177247\n",
      " 0.00176391 0.00174575 0.00174084 0.00173493 0.00172713 0.00172038\n",
      " 0.00171324 0.00169736 0.00169107 0.00168815 0.00167718 0.00166958\n",
      " 0.0016606  0.00165005 0.00164831 0.00163445 0.00162676 0.00162542\n",
      " 0.00161713 0.00161272 0.00159771 0.00159238 0.00159037 0.00158957\n",
      " 0.00156833 0.00156387 0.00155943 0.00155314 0.00154517 0.00154178\n",
      " 0.00153878 0.00153156 0.00152027 0.00151681 0.00150908 0.00150907\n",
      " 0.00150429 0.00148962 0.00148561 0.00147865 0.00147801 0.00146682\n",
      " 0.00146001 0.00145723 0.00144468 0.00143961 0.00143002 0.00142921\n",
      " 0.0014208  0.00141638 0.00141579 0.00140818 0.0014034  0.00139324\n",
      " 0.00139056 0.00138567 0.00137493 0.00137155 0.00136727 0.00135985\n",
      " 0.00135444 0.00134977 0.00134562 0.00133558 0.00133122 0.00132535\n",
      " 0.00132194 0.00131758 0.00131066 0.00130911 0.00130505 0.00129728\n",
      " 0.00129571 0.00128405 0.00127616 0.00127324 0.00126559 0.00125941\n",
      " 0.00125722 0.00125095 0.00124375 0.00124088 0.00123686 0.00123166\n",
      " 0.00122136 0.00121416 0.00120729 0.00119822 0.00119425 0.0011902\n",
      " 0.00118219 0.00117517 0.00117452 0.0011669  0.00115835 0.00114922\n",
      " 0.00114639 0.00113962 0.00113862 0.00113157 0.00112449]\n",
      "0.6274704549280454\n",
      "[7.18722998 4.37716265 3.87707769 3.80401495 3.46987465 3.38992696\n",
      " 3.31319188 3.23081401 3.19295868 3.11176267 3.0479269  2.97464527\n",
      " 2.94181282 2.84376785 2.7792728  2.72052017 2.67818072 2.66871722\n",
      " 2.62671166 2.6211839  2.59772293 2.58190277 2.54773577 2.50549056\n",
      " 2.47514125 2.46249764 2.44458218 2.44081919 2.38195771 2.37595715\n",
      " 2.36213857 2.33646299 2.31897222 2.29462272 2.28988032 2.27951064\n",
      " 2.2696032  2.25224959 2.21275983 2.20157946 2.18941753 2.18229454\n",
      " 2.16700863 2.15629757 2.14384191 2.13417342 2.10461423 2.09580568\n",
      " 2.08809633 2.08457671 2.06118326 2.05524985 2.03983521 2.02947381\n",
      " 2.0156339  2.00808902 2.00484508 1.98892489 1.9848698  1.97080544\n",
      " 1.96433732 1.95685115 1.94851579 1.94618356 1.93878407 1.92354149\n",
      " 1.91631125 1.90417728 1.8957369  1.88786307 1.8786183  1.86911358\n",
      " 1.86110158 1.85717919 1.84914318 1.84466386 1.83680751 1.83047136\n",
      " 1.8257132  1.82052779 1.81602147 1.80661829 1.80431053 1.79331989\n",
      " 1.79183982 1.77843174 1.77363887 1.76688172 1.76225481 1.75901716\n",
      " 1.7472136  1.74547341 1.73138953 1.72112568 1.71500666 1.71000327\n",
      " 1.70948057 1.69627556 1.69249592 1.69011496 1.68727252 1.67602962\n",
      " 1.67226219 1.66647038 1.66276613 1.65396201 1.65116958 1.64756305\n",
      " 1.63885047 1.6351727  1.63216299 1.62859852 1.62272231 1.6199386\n",
      " 1.61197651 1.60973638 1.60686067 1.6019874  1.59937235 1.59485625\n",
      " 1.58767203 1.58485923 1.58104529 1.57976164 1.57568875 1.57192692\n",
      " 1.56637507 1.55945183 1.55681473 1.55449401 1.55349754 1.54521608\n",
      " 1.54228311 1.53494218 1.53323466 1.52873209 1.52685008 1.52467317\n",
      " 1.52095301 1.51360868 1.51136655 1.50844406 1.50501843 1.50295063\n",
      " 1.4990343  1.49203042 1.48921439 1.48844592 1.48323386 1.47971939\n",
      " 1.47588973 1.47125412 1.47030967 1.46441287 1.46089164 1.4600098\n",
      " 1.45645421 1.45430045 1.44757356 1.4451948  1.44418574 1.44381833\n",
      " 1.43474477 1.43241517 1.43075667 1.42734688 1.4239681  1.42213184\n",
      " 1.42056782 1.41722678 1.4129636  1.41085354 1.40692287 1.40682537\n",
      " 1.40490821 1.39769447 1.39580527 1.39275219 1.3922484  1.38695654\n",
      " 1.38382998 1.38275183 1.37645068 1.37404564 1.36964869 1.36905779\n",
      " 1.36514341 1.36291003 1.36262579 1.35901456 1.35666126 1.35220907\n",
      " 1.35071654 1.34873489 1.34301621 1.34134439 1.33943739 1.33542613\n",
      " 1.33276808 1.33056349 1.3284238  1.32349714 1.32147413 1.31838175\n",
      " 1.31677435 1.31457724 1.31126584 1.31041624 1.30838878 1.30434815\n",
      " 1.30356031 1.29770436 1.29424264 1.29220091 1.28853219 1.28520775\n",
      " 1.2841347  1.28089417 1.27725376 1.27569374 1.27362989 1.27093976\n",
      " 1.26565419 1.26203966 1.25829658 1.25355624 1.25168127 1.24946418\n",
      " 1.24542796 1.24144044 1.24113263 1.23706028 1.2325742  1.22773662\n",
      " 1.22629484 1.22251701 1.22209418 1.21818685 1.21495199]\n",
      "(239, 2023)\n"
     ]
    }
   ],
   "source": [
    "#lsa를 쓰면?\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=239, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_train_tfidf)  \n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "print(svd.singular_values_) \n",
    "print(svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.763\n",
      "Test set score: 0.654\n"
     ]
    }
   ],
   "source": [
    "X_train_svd = svd.transform(X_train_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SVD_clf = LogisticRegression()\n",
    "SVD_clf.fit(X_train_svd, y_train)\n",
    "print('Train set score: {:.3f}'.format(SVD_clf.score(X_train_svd, y_train)))\n",
    "print('Test set score: {:.3f}'.format(SVD_clf.score(X_test_svd, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1370, 1584)\n",
      "Test set dimension: (457, 1584)\n",
      "Train set score: 0.801\n",
      "Test set score: 0.685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(tokenizer=twit_tokenizer2, min_df=2).fit(X_train) #tfidf와 동일하게 max_feature를 제한하여 학습\n",
    "X_train_cv = cv.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train_cv, y_train)\n",
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train)))\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "#메일 내용에서 hint가 되는 부분을 삭제 - 순수하게 내용만으로\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 2034\n",
      "test set size: 1353\n",
      "selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "train labels: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "print('train set size:', len(newsgroups_train.data))\n",
    "print('test set size:', len(newsgroups_test.data))\n",
    "print('selected categories:', newsgroups_train.target_names)\n",
    "print('train labels:', set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Train set text samples: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "##Train set label smaples: 1\n",
      "##Test set text samples: TRry the SKywatch project in  Arizona.\n",
      "##Test set label smaples: 2\n"
     ]
    }
   ],
   "source": [
    "print('##Train set text samples:', newsgroups_train.data[0])\n",
    "print('##Train set label smaples:', newsgroups_train.target[0])\n",
    "print('##Test set text samples:', newsgroups_test.data[0])\n",
    "print('##Test set label smaples:', newsgroups_test.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 11483)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
    "                        decode_error ='ignore', \n",
    "                        lowercase=True, \n",
    "                        stop_words = stopwords.words('english'), \n",
    "                        max_df=0.5,\n",
    "                        min_df=2).fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.952\n",
      "Test set score: 0.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26550)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
    "                        decode_error ='ignore', \n",
    "                        lowercase=True, \n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        ngram_range=(1, 2),\n",
    "                        max_df=0.5,\n",
    "                        min_df=2).fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'cause can't\", \"'em better\", \"'expected errors'\", \"'karla' next\", \"'nodis' password\", \"'official doctrine\", \"'ok see\", \"'sci astro'\", \"'what's moonbase\", 'aas american']\n"
     ]
    }
   ],
   "source": [
    "bigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 1]\n",
    "print(bigram_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.960\n",
      "Test set score: 0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 32943)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
    "                        decode_error ='ignore', \n",
    "                        lowercase=True, \n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        ngram_range=(1, 3),\n",
    "                        max_df=0.5,\n",
    "                        min_df=2).fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'em better shots\", \"'expected errors' basically\", \"'karla' next one\", \"'nodis' password also\", \"'official doctrine think\", \"'ok see warning\", \"'what's moonbase good\", 'aas american astronautical', 'ability means infallible', 'able accept donations']\n"
     ]
    }
   ],
   "source": [
    "trigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 2]\n",
    "print(trigram_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.960\n",
      "Test set score: 0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.976\n",
      "Test set score: 0.774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.761\n",
      "Test set score: 0.696\n",
      "Used features count: 248 out of 32943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
    "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.974\n",
      "Test set score: 0.758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(X_train_tfidf, y_train) \n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
